{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.8;\"> بوستان و گلستان سعدی\n",
    "        </h1>\n",
    "        <p dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        در این پروژه موضوع بوستان و گلستان سعدی انتخاب شده است.\n",
    "          <br>\n",
    "         دیتای مورد نیاز برای این پروژه، کتاب بوستان و گلستان سعدی است که از سایت <a href=\"https://ganjoor.net//saadi/\">گنجور</a>\n",
    "          با استفاده از کرالری که نوشتیم و در فایل تمرین است، جمع‌آوری شده است.\n",
    "        <br>\n",
    "        هر حکایت یا شعر از این دو کتاب، در یک فایل .txt ذخیره شده است و در این قسمت داده ها از فایل‌های ذخیره شده، خوانده میشوند.\n",
    "            \n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <p dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "    ابتدا باید پکیج های مورد نیاز را نصب کنیم.\n",
    "</html>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.3)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: hazm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.7.0)\n",
      "Requirement already satisfied: libwapiti>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from hazm) (0.2.1)\n",
      "Requirement already satisfied: nltk==3.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk==3.3->hazm) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: future in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.18.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: bs4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install hazm\n",
    "!pip install future\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import codecs\n",
    "import tqdm\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "from __future__ import unicode_literals\n",
    "import random\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boostanPath = \"./Boostan\"\n",
    "golestanPath = \"./Golestan\"\n",
    "NumOfBoostanFiles = len([name for name in os.listdir(boostanPath)])\n",
    "NumOfGolestanFiles = len([name for name in os.listdir(golestanPath)])\n",
    "NumOfBoostanFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <p dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        نام فایل های بوستان و گلستان سعدی را در دو آرایه ذخیره میکنیم تا بعدا از آنها استفاده کنیم.\n",
    "\n",
    "</html> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <p dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        فایل های بوستان\n",
    "</html> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sinaelahimanesh/Desktop/HW3/MIR-Saadi\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sinaelahimanesh/Desktop/HW3/MIR-Saadi\n",
      "230\n",
      "/Users/sinaelahimanesh/Desktop/HW3/MIR-Saadi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bab1sh17.txt',\n",
       " 'bab3sh1.txt',\n",
       " 'bab4sh8.txt',\n",
       " 'bab8sh14.txt',\n",
       " 'bab8sh8.txt',\n",
       " 'bab1sh4.txt',\n",
       " 'bab9sh11.txt',\n",
       " 'bab4sh26.txt',\n",
       " 'bab9sh10.txt',\n",
       " 'bab4sh27.txt']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd()) \n",
    "boostan_folder_path = \"./Boostan\"\n",
    "\n",
    "boostan_file_name = []\n",
    "golestan_file_name = []\n",
    "\n",
    "os.chdir(boostan_folder_path)\n",
    "print(len(os.listdir()))\n",
    "for filename in os.listdir():\n",
    "    if os.path.isfile(filename):\n",
    "        boostan_file_name.append(filename)\n",
    "\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd()) \n",
    "boostan_file_name[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <p dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        فایل های گلستان\n",
    "</html> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sinaelahimanesh/Desktop/HW3/MIR-Saadi\n",
      "291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gbab8sh29.txt',\n",
       " 'gbab8sh15.txt',\n",
       " 'gbab2sh6.txt',\n",
       " 'gbab8sh100.txt',\n",
       " 'dibache.txt',\n",
       " 'gbab1sh16.txt',\n",
       " 'gbab1sh17.txt',\n",
       " 'gbab8sh101.txt',\n",
       " 'gbab2sh7.txt',\n",
       " 'gbab8sh14.txt']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd())  \n",
    "golestan_folder_path = \"./Golestan\"\n",
    "os.chdir(golestan_folder_path)\n",
    "print(len(os.listdir()))\n",
    "for filename in os.listdir():\n",
    "    if os.path.isfile(filename):\n",
    "        golestan_file_name.append(filename)\n",
    "os.chdir(\"..\")\n",
    "golestan_file_name[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make array for golestan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['متکلّم را تا کسی عیب نگیرد، سخنش صلاح نپذیرد.',\n",
       " 'مشو غرّه بر حُسن ِ گفتار ِ خویش',\n",
       " 'به تحسین نادان و پندار خویش',\n",
       " '',\n",
       " 'بر عجز دشمن رحمت مکن که اگر قادر شود بر تو نبخشاید.',\n",
       " 'دشمن چو بینی ناتوان لاف از بروت خود مزن',\n",
       " 'مغزیست در هر استخوان مردیست در هر پیرهن',\n",
       " '',\n",
       " 'زاهدی مهمان پادشاهی بود. چون به طعام بنشستند کمتر از آن خورد که ارادت او بود و چون به نماز برخاستند بیش از آن کرد که عادت او، تا ظنّ صلاحیت در حق او زیادت کنند.',\n",
       " 'ترسم نرسی به کعبه، ای اعرابی ']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golestan = ''\n",
    "\n",
    "for file in golestan_file_name:\n",
    "    f = open(\"./Golestan/\"+file, \"r\", encoding=\"utf-8\")\n",
    "    text = f.read()\n",
    "    golestan += text\n",
    "    golestan += \"\\n\"\n",
    "\n",
    "golestan_poems = golestan.split(\"\\n\")\n",
    "golestan_poems[0:10]\n",
    "# golestan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make array for boostan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مگو جاهی از سلطنت بیش نیست',\n",
       " 'که ایمن\\u200cتر از ملک درویش نیست',\n",
       " 'سبکبار مردم سبک\\u200cتر روند',\n",
       " 'حق این است و صاحبدلان بشنوند',\n",
       " 'تهیدست تشویش نانی خورد',\n",
       " 'جهانبان به قدر جهانی خورد',\n",
       " 'گدا را چو حاصل شود نان شام',\n",
       " 'چنان خوش بخسبد که سلطان شام',\n",
       " 'غم و شادمانی به سر می\\u200cرود',\n",
       " 'به مرگ این دو از سر به در می\\u200cرود']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boostan = ''\n",
    "\n",
    "for file in boostan_file_name:\n",
    "    fi = open(\"./Boostan/\"+file, \"r\", encoding=\"utf-8\")\n",
    "    text = fi.read()\n",
    "    boostan += text\n",
    "    boostan += \"\\n\"\n",
    "\n",
    "boostan_poems = boostan.split(\"\\n\")\n",
    "boostan_poems[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start preprocessing Boostan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize boostan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مگو جاهی از سلطنت بیش نیست',\n",
       " 'که ایمن\\u200cتر از ملک درویش نیست',\n",
       " 'سبکبار مردم سبک\\u200cتر روند',\n",
       " 'حق این است و صاحبدلان بشنوند',\n",
       " 'تهیدست تشویش نانی خورد',\n",
       " 'جهانبان به قدر جهانی خورد',\n",
       " 'گدا را چو حاصل شود نان شام',\n",
       " 'چنان خوش بخسبد که سلطان شام',\n",
       " 'غم و شادمانی به سر می\\u200cرود',\n",
       " 'به مرگ این دو از سر به در می\\u200cرود']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "normalized_boostan_poems = []\n",
    "for p in boostan_poems:\n",
    "    normalized_boostan_poems.append(normalizer.normalize(p))\n",
    "normalized_boostan_poems[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatize and remove stopwords boostan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مگو جاه سلطنت', 'ایمن ملک درویش', 'سبکبار مردم سبک', 'حق صاحبدلان شنید#شنو', 'تهیدست تشویش نان خورد#خور', 'جهانبان قدر جهانی خورد#خور', 'گدا چو حاصل نان شام', 'چنان خوش بخسبد سلطان شام', 'غم شادمان سر', 'مرگ سر']\n",
      "[['مگو', 'جاه', 'سلطنت'], ['ایمن', 'ملک', 'درویش'], ['سبکبار', 'مردم', 'سبک'], ['حق', 'صاحبدلان', 'شنید#شنو'], ['تهیدست', 'تشویش', 'نان', 'خورد#خور'], ['جهانبان', 'قدر', 'جهانی', 'خورد#خور'], ['گدا', 'چو', 'حاصل', 'نان', 'شام'], ['چنان', 'خوش', 'بخسبد', 'سلطان', 'شام'], ['غم', 'شادمان', 'سر'], ['مرگ', 'سر']]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lemmatize_boostan_poems = []\n",
    "all_cleaned_tokens = []\n",
    "stopwords = [normalizer.normalize(x.strip()) for x in stopwords_list()]+[\"-\", \"/\", \"ز\", '\\ufeff', \"]\", \"[\", \"?\", \"؟\"]\n",
    "\n",
    "mesra_boostan = []\n",
    "mesra_boostan_tokenized = []\n",
    "\n",
    "for p in normalized_boostan_poems:\n",
    "    all_tokens =  word_tokenize(p)\n",
    "    all_tokens_nonstop = [t for t in (all_tokens) if t not in stopwords]\n",
    "    all_tokens_nonstop = [t for t in (all_tokens_nonstop) if len(t)>1]\n",
    "\n",
    "    all_tokens_lemm = []\n",
    "    for i in all_tokens_nonstop:\n",
    "        all_tokens_lemm.append(lemmatizer.lemmatize(i))\n",
    "    \n",
    "    mesra_boostan.append(' '.join(all_tokens_lemm))\n",
    "    # print(all_tokens_nonstop[0])\n",
    "    mesra_boostan_tokenized.append(all_tokens_lemm)\n",
    "\n",
    "print(mesra_boostan[0:10])\n",
    "print(mesra_boostan_tokenized[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start preprocessing Golestan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize golestan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['متکلم را تا کسی عیب نگیرد، سخنش صلاح نپذیرد.',\n",
       " 'مشو غره بر حسن  گفتار  خویش',\n",
       " 'به تحسین نادان و پندار خویش',\n",
       " '',\n",
       " 'بر عجز دشمن رحمت مکن که اگر قادر شود بر تو نبخشاید.',\n",
       " 'دشمن چو بینی ناتوان لاف از بروت خود مزن',\n",
       " 'مغزیست در هر استخوان مردیست در هر پیرهن',\n",
       " '',\n",
       " 'زاهدی مهمان پادشاهی بود. چون به طعام بنشستند کمتر از آن خورد که ارادت او بود و چون به نماز برخاستند بیش از آن کرد که عادت او، تا ظن صلاحیت در حق او زیادت کنند.',\n",
       " 'ترسم نرسی به کعبه، ای اعرابی ']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "normalized_golestan_poems = []\n",
    "for p in golestan_poems:\n",
    "    normalized_golestan_poems.append(normalizer.normalize(p))\n",
    "normalized_golestan_poems[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['متکلم عیب گرفت#گیر سخن صلاح پذیرفت#پذیر', 'مشو غره حسن گفتار', 'تحسین نادان پندار', '', 'عجز دشمن رحمت مکن قادر نبخشاید', 'دشمن چو بینی ناتوان لاف برو مزن', 'مغزیست استخوان مردیست پیرهن', '', 'زاهد مهمان پادشاه طعام بنشستند کمتر خورد#خور ارادت نماز برخاستند عادت ظن صلاحیت حق زیاد', 'ترسید#ترس رسید#رس کعبه ای اعرابی']\n",
      "[['متکلم', 'عیب', 'گرفت#گیر', 'سخن', 'صلاح', 'پذیرفت#پذیر'], ['مشو', 'غره', 'حسن', 'گفتار'], ['تحسین', 'نادان', 'پندار'], [], ['عجز', 'دشمن', 'رحمت', 'مکن', 'قادر', 'نبخشاید'], ['دشمن', 'چو', 'بینی', 'ناتوان', 'لاف', 'برو', 'مزن'], ['مغزیست', 'استخوان', 'مردیست', 'پیرهن'], [], ['زاهد', 'مهمان', 'پادشاه', 'طعام', 'بنشستند', 'کمتر', 'خورد#خور', 'ارادت', 'نماز', 'برخاستند', 'عادت', 'ظن', 'صلاحیت', 'حق', 'زیاد'], ['ترسید#ترس', 'رسید#رس', 'کعبه', 'ای', 'اعرابی']]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lemmatize_golestan_poems = []\n",
    "all_cleaned_tokens = []\n",
    "stopwords = [normalizer.normalize(x.strip()) for x in stopwords_list()]+[\"-\", \"/\", \"ز\", '\\ufeff', \"]\", \"[\", \"?\", \"؟\"]\n",
    "\n",
    "mesra_golestan = []\n",
    "mesra_golestan_tokenized = []\n",
    "\n",
    "for p in normalized_golestan_poems:\n",
    "    all_tokens =  word_tokenize(p)\n",
    "    all_tokens_nonstop_g = [t for t in (all_tokens) if t not in stopwords]\n",
    "    all_tokens_nonstop_g = [t for t in (all_tokens_nonstop_g) if len(t)>1]\n",
    "\n",
    "    all_tokens_lemm = []\n",
    "    for i in all_tokens_nonstop_g:\n",
    "        all_tokens_lemm.append(lemmatizer.lemmatize(i))\n",
    "    \n",
    "    mesra_golestan.append(' '.join(all_tokens_lemm))\n",
    "    # print(all_tokens_nonstop[0])\n",
    "    mesra_golestan_tokenized.append(all_tokens_lemm)\n",
    "\n",
    "print(mesra_golestan[0:10])\n",
    "print(mesra_golestan_tokenized[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfromers-Based Model (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.8;\">مدل های امبدینگ مبتنی بر\n",
    "        transformer</h1>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            یکی از روش‌های مناسب امبدینگ، روش‌های مبتنی بر\n",
    "            transformer\n",
    "            ها هستند و به نوعی مدل‌هایی جدید هستند.\n",
    "            به عنوان نمونه می‌توان از مدل زبانی \n",
    "            BERT\n",
    "            استفاده نمود که مدلی مناسب در ترنسفورمرها می‌باشد.\n",
    "            <br>\n",
    "            حال در این زیربخش سعی داریم با استفاده از \n",
    "            BERT\n",
    "            یک امبدینگ مناسب برای آن انجام دهیم و در نهایت هم موتور جستجوگری برای بوستان و گلستان ایجاد نماییم.\n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentence_transformers in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (4.19.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (0.11.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (1.21.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (1.7.0)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (3.3)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (0.1.96)\n",
      "Requirement already satisfied: huggingface-hub in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (4.0.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk->sentence_transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (3.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision->sentence_transformers) (8.4.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.0.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در این مرحله کافیست مدل \n",
    "            BERT\n",
    "            را از\n",
    "            huggingface\n",
    "            لود کنیم و سپس مصراع‌های بوستان و گلستان که قبل‌تر\n",
    "            روی آنها پیش ‌پردازش کرده بودیم،\n",
    "            با استفاده از \n",
    "            BERT\n",
    "            انکود کنیم.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8359, 768)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesra_boostan_embeddings = model.encode(mesra_boostan) \n",
    "mesra_boostan_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3429, 768)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesra_golestan_embeddings = model.encode(mesra_golestan) \n",
    "mesra_golestan_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در مرحله بعد یک\n",
    "            query\n",
    "            تعریف می‌کنیم و به این ترتیب برای سادگی کار یک تابع برای پیش‌پردازش تعریف می‌نماییم.\n",
    "            که این تابع کارش این است که یک کوئری ورودی را می‌گیرد و ابتدا آن را نرمالایز می‌کند، سپس آن را به توکن‌هایی تبدیل می‌کند و فرآیند\n",
    "            lemmatization \n",
    "            را روی آن اعمال می‌کند.\n",
    "            در نهایت دوباره آن را به یک جمله تبدیل می‌کند و بر می‌گرداند.\n",
    "            <br>\n",
    "            در ادامه از این تابع استفاده نموده‌ایم و کوئری‌ای که داشته‌ایم را پیش‌پردازش کرده‌ایم و سپس شباهت \n",
    "            Cosine Similarity\n",
    "            را برای آن محاسبه کرده‌ایم که بردار نهایی خروجی‌ای است که حاصل این ضرب داخلی بردار هاست.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'همان‌ گونه عذرت بباید خواستن تقصیر را'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    normalizer = Normalizer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    stopwords = [normalizer.normalize(x.strip()) for x in stopwords_list()]+[\"-\", \"/\", \"ز\", '\\ufeff', \"]\", \"[\", \"?\", \"؟\"]\n",
    "    query_normalized = normalizer.normalize(query)\n",
    "    all_tokens = word_tokenize(query_normalized)\n",
    "    all_tokens_nonstop = [t for t in (all_tokens) if t not in stopwords]\n",
    "    all_tokens_nonstop = [t for t in (all_tokens_nonstop) if len(t)>1]\n",
    "\n",
    "    all_tokens_lemm = []\n",
    "    for i in all_tokens_nonstop:\n",
    "        all_tokens_lemm.append(lemmatizer.lemmatize(i))\n",
    "        \n",
    "    return ' '.join(all_tokens_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در آخر کافیست یک تابع تعریف کنیم که در یک آرایه به طول نامشخص\n",
    "            بتواند \n",
    "            k\n",
    "            عنصر با بیشترین مقدار آن را بیابد و به همان ترتیب بتواند اشعار متناظر با آنها را به عنوان پاسخ برگرداند.\n",
    "            از طرفی دو تابع با نام‌های\n",
    "            search_in_boostan\n",
    "            و\n",
    "            search_in_golestan\n",
    "            می‌نویسیم که یک \n",
    "            query\n",
    "            و\n",
    "            یک عدد\n",
    "            k\n",
    "            می‌گیرد و به این ترتیب ابتدا کوئری را پیش‌پردازش می‌کند و سپس\n",
    "            فرآیند\n",
    "            embedding\n",
    "            را روی آن انجام می‌دهد و به کمک مدل\n",
    "            BERT\n",
    "            آن را امبد می‌کند و در نهایت هم با استفاده از\n",
    "            Cosine Similarity\n",
    "            سعی می‌کند بردار شباهت آنها را بیابد و با استفاده از تابع کمکی\n",
    "            find_k_most_relevant\n",
    "            خروجی را تولید می‌کند.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_most_relevant(query_similarity_vector, boostan_poems, k):\n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        max_value = max(query_similarity_vector)\n",
    "        max_index = query_similarity_vector.index(max_value)\n",
    "        results.append(boostan_poems[max_index])\n",
    "        query_similarity_vector[max_index] = -1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_boostan(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_embedding = model.encode(query) \n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_embedding],\n",
    "        mesra_boostan_embeddings\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, boostan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['همین نکته بس عذر تقصیر ما',\n",
       " 'ببایدت عذر خطا خواستن',\n",
       " 'کنون بایدت عذر تقصیر گفت',\n",
       " 'بگفت این قدر ستر و آسایش است',\n",
       " 'برو عذر تقصیر طاعت بیار',\n",
       " 'در اقبال و تأیید بوبکر سعد',\n",
       " 'خورنده که خیرش برآید ز دست',\n",
       " 'براندیش از افتان و خیزان تب',\n",
       " 'که فرزند خویشت برآید تباه',\n",
       " 'حق از بهر باطل نشاید نهفت']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_boostan(query, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_golestan(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_embedding = model.encode(query) \n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_embedding],\n",
    "        mesra_golestan_embeddings\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, golestan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['بنده همان به که ز تقصیر خویش',\n",
       " 'تو بر سر قدر خویشتن باش و وقار ',\n",
       " 'گر التفات خداوندیش بیاراید',\n",
       " 'و آدمی\\u200cبچه ندارد خبر و عقل و تمیز',\n",
       " 'که در طویله نامردمم بباید ساخت',\n",
       " 'افلاس عنان از کف تقوی بستاند',\n",
       " 'نترسد آن که بر افتادگان نبخشاید',\n",
       " 'که هر خاری به تسبیحش زبانیست',\n",
       " 'گفت: ترسم که بینا شود و دخترم را طلاق دهد.',\n",
       " 'ور وزیر از خدا بترسیدی ']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_golestan(query, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.8;\">مدل های امبدینگ مبتنی بر\n",
    "        میانگین وزن‌دار بردارهای تعبیه</h1>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در این قسمت سعی داریم از مدل \n",
    "            FastText\n",
    "            استفاده نماییم و به این ترتیب برای هر کلمه یک \n",
    "            embedding\n",
    "            داریم. \n",
    "            در واقع می‌توان گفت برخلاف روش استفاده از \n",
    "            BERT\n",
    "            که امبدینگ به صورت\n",
    "            wave to vector\n",
    "            بوده است، این روش یک متد\n",
    "            word to vector\n",
    "            است و به این ترتیب برای هر کلمه یک امبدینگ وجود دارد.\n",
    "            <br>\n",
    "            در ادامه ابتدا کتابخانه‌های مورد نیاز را\n",
    "            import\n",
    "            کرده‌ایم و سپس برای\n",
    "            trainig\n",
    "            تمام فایل‌هایی که به عنوان اشعار بوستان داشتیم را با هم تجمیع می‌کنیم و تمام فایل‌های گلستان را هم یکی می‌کنیم.\n",
    "            در مرحله بعد مدل را روی این فایل‌ها \n",
    "            train\n",
    "            می‌کنیم.\n",
    "            در ادامه مدل را ذخیره می‌کنیم که به صورت یک فایل\n",
    "            bin\n",
    "            ذخیره می‌شود.\n",
    "            \n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('boostan_all.txt', mode='wt', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(mesra_boostan))\n",
    "with open('golestan_all.txt', mode='wt', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(mesra_golestan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  1315\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  127933 lr:  0.000000 avg.loss:  3.607142 ETA:   0h 0m 0s\n",
      "Read 0M words\n",
      "Number of words:  953\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   83931 lr:  0.000000 avg.loss:  3.138286 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model_boostan = fasttext.train_unsupervised('boostan_all.txt')\n",
    "model_golestan = fasttext.train_unsupervised('golestan_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_boostan.save_model(\"boostan_skipgram_model.bin\")\n",
    "model_golestan.save_model(\"golestan_skipgram_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در ادامه دو تابع کاربردی تعریف می‌کنیم. یکی برای محاسبه ی \n",
    "            average vector\n",
    "            برای یک مصراع و به این ترتیب یک مصراع را می‌گیرد و می‌تواند تک تک کلمات آن را\n",
    "            امبد کند و در ادامه میانگین آن را محاسبه می‌نماید.\n",
    "            تابع دوم کارش این است که برای تمام مصراع‌های یک شعر این فرآیند را انجام دهد و یک نام فایل و مدل می‌گیرد\n",
    "            و می‌تواند با استفاده از تابع اول برای تک‌تک مصراع‌ها این فرآیند را انجام دهد.\n",
    "            در ادامه این مورد را برای هر دو سند بوستان و گلستان محاسبه می‌نماییم.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_avg_vector(model, mesra):\n",
    "    all_tokens = mesra.split(' ')\n",
    "    tokens_vectors = []\n",
    "    for word in all_tokens:\n",
    "        tokens_vectors.append(model.get_word_vector(word))\n",
    "\n",
    "    return np.mean(np.array(tokens_vectors), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_text_avg_vectors(model, file_path):\n",
    "    mesra_list = []\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        mesra_list = [line.rstrip() for line in lines]\n",
    "\n",
    "    mesra_avg_vectors = []\n",
    "    \n",
    "    for mesra in mesra_list:\n",
    "        mesra_avg_vect = calculate_text_avg_vector(model, mesra)\n",
    "        mesra_avg_vectors.append(mesra_avg_vect)\n",
    "\n",
    "    return mesra_avg_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8358\n"
     ]
    }
   ],
   "source": [
    "boostan_avg_vector = calculate_all_text_avg_vectors(model_boostan, 'boostan_all.txt')\n",
    "print(len(boostan_avg_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3428\n"
     ]
    }
   ],
   "source": [
    "golestan_avg_vector = calculate_all_text_avg_vectors(model_golestan, 'golestan_all.txt')\n",
    "print(len(golestan_avg_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در مرحله آخر کافیست تابع‌هایی برای این جستجو تعریف نماییم و با استفاده از آنها همانند بخش قبل\n",
    "            می‌توانیم یک کوئری و یک \n",
    "            k\n",
    "            بگیریم و \n",
    "            k\n",
    "            نزدیک ترین سند را در پاسخ برگردانیم.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_boostan_fasttext(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_vector = calculate_text_avg_vector(model_boostan, query)\n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_vector],\n",
    "        boostan_avg_vector\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, boostan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_golestan_fasttext(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_vector = calculate_text_avg_vector(model_golestan, query)\n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_vector],\n",
    "        golestan_avg_vector\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, golestan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'همان‌ گونه عذرت بباید خواستن تقصیر را'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ملامت کنی گفتش ای باد دست',\n",
       " 'برو عذر تقصیر طاعت بیار',\n",
       " 'زبان در دهان است عذری بیار',\n",
       " 'به فریاد خواهان باران شدند',\n",
       " 'که خورد اندر آن روز چندان شراب',\n",
       " 'به شیرین زبانی توان برد گوی',\n",
       " 'نه هر بار خرما توان خورد و برد',\n",
       " 'که پایابم از دست دشمن نماند',\n",
       " 'همه عمر از اینان چه دیدی خوشی',\n",
       " 'به چندان که در دستت افتد بساز']"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_boostan_fasttext(query, k)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
