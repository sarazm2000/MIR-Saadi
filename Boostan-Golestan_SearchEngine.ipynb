{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.8;\"> بوستان و گلستان سعدی\n",
    "        </h1>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        در این پروژه موضوع بوستان و گلستان سعدی انتخاب شده است.\n",
    "          <br>\n",
    "         دیتای مورد نیاز برای این پروژه، کتاب بوستان و گلستان سعدی است که از سایت <a href=\"https://ganjoor.net//saadi/\">گنجور</a>\n",
    "          با استفاده از کرالری که نوشتیم و در فایل تمرین است، جمع‌آوری شده است.\n",
    "        <br>\n",
    "        هر حکایت یا شعر از این دو کتاب، در یک فایل .txt ذخیره شده است و در این قسمت داده ها از فایل‌های ذخیره شده، خوانده میشوند.\n",
    "            \n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "    ابتدا باید پکیج های مورد نیاز را نصب کنیم.\n",
    "</html>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.3)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: hazm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.7.0)\n",
      "Requirement already satisfied: libwapiti>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from hazm) (0.2.1)\n",
      "Requirement already satisfied: nltk==3.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk==3.3->hazm) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: future in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.18.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: bs4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install hazm\n",
    "!pip install future\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import codecs\n",
    "import tqdm\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "from __future__ import unicode_literals\n",
    "import random\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boostanPath = \"./Boostan\"\n",
    "golestanPath = \"./Golestan\"\n",
    "NumOfBoostanFiles = len([name for name in os.listdir(boostanPath)])\n",
    "NumOfGolestanFiles = len([name for name in os.listdir(golestanPath)])\n",
    "NumOfBoostanFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        نام فایل های بوستان و گلستان سعدی را در دو آرایه ذخیره میکنیم تا بعدا از آنها استفاده کنیم.\n",
    "\n",
    "</html> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        فایل های بوستان\n",
    "</html> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sinaelahimanesh/Desktop/HW3_2/MIR-Saadi\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sinaelahimanesh/Desktop/HW3_2/MIR-Saadi\n",
      "230\n",
      "/Users/sinaelahimanesh/Desktop/HW3_2/MIR-Saadi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bab1sh17.txt',\n",
       " 'bab3sh1.txt',\n",
       " 'bab4sh8.txt',\n",
       " 'bab8sh14.txt',\n",
       " 'bab8sh8.txt',\n",
       " 'bab1sh4.txt',\n",
       " 'bab9sh11.txt',\n",
       " 'bab4sh26.txt',\n",
       " 'bab9sh10.txt',\n",
       " 'bab4sh27.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd()) \n",
    "boostan_folder_path = \"./Boostan\"\n",
    "\n",
    "boostan_file_name = []\n",
    "golestan_file_name = []\n",
    "\n",
    "os.chdir(boostan_folder_path)\n",
    "print(len(os.listdir()))\n",
    "for filename in os.listdir():\n",
    "    if os.path.isfile(filename):\n",
    "        boostan_file_name.append(filename)\n",
    "\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd()) \n",
    "boostan_file_name[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        فایل های گلستان\n",
    "</html> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sinaelahimanesh/Desktop/HW3_2/MIR-Saadi\n",
      "291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gbab8sh29.txt',\n",
       " 'gbab8sh15.txt',\n",
       " 'gbab2sh6.txt',\n",
       " 'gbab8sh100.txt',\n",
       " 'dibache.txt',\n",
       " 'gbab1sh16.txt',\n",
       " 'gbab1sh17.txt',\n",
       " 'gbab8sh101.txt',\n",
       " 'gbab2sh7.txt',\n",
       " 'gbab8sh14.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd())  \n",
    "golestan_folder_path = \"./Golestan\"\n",
    "os.chdir(golestan_folder_path)\n",
    "print(len(os.listdir()))\n",
    "for filename in os.listdir():\n",
    "    if os.path.isfile(filename):\n",
    "        golestan_file_name.append(filename)\n",
    "os.chdir(\"..\")\n",
    "golestan_file_name[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make array for golestan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['متکلّم را تا کسی عیب نگیرد، سخنش صلاح نپذیرد.',\n",
       " 'مشو غرّه بر حُسن ِ گفتار ِ خویش',\n",
       " 'به تحسین نادان و پندار خویش',\n",
       " '',\n",
       " 'بر عجز دشمن رحمت مکن که اگر قادر شود بر تو نبخشاید.',\n",
       " 'دشمن چو بینی ناتوان لاف از بروت خود مزن',\n",
       " 'مغزیست در هر استخوان مردیست در هر پیرهن',\n",
       " '',\n",
       " 'زاهدی مهمان پادشاهی بود. چون به طعام بنشستند کمتر از آن خورد که ارادت او بود و چون به نماز برخاستند بیش از آن کرد که عادت او، تا ظنّ صلاحیت در حق او زیادت کنند.',\n",
       " 'ترسم نرسی به کعبه، ای اعرابی ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golestan = ''\n",
    "\n",
    "for file in golestan_file_name:\n",
    "    f = open(\"./Golestan/\"+file, \"r\", encoding=\"utf-8\")\n",
    "    text = f.read()\n",
    "    golestan += text\n",
    "    golestan += \"\\n\"\n",
    "\n",
    "golestan_poems = golestan.split(\"\\n\")\n",
    "golestan_poems[0:10]\n",
    "# golestan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make array for boostan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مگو جاهی از سلطنت بیش نیست',\n",
       " 'که ایمن\\u200cتر از ملک درویش نیست',\n",
       " 'سبکبار مردم سبک\\u200cتر روند',\n",
       " 'حق این است و صاحبدلان بشنوند',\n",
       " 'تهیدست تشویش نانی خورد',\n",
       " 'جهانبان به قدر جهانی خورد',\n",
       " 'گدا را چو حاصل شود نان شام',\n",
       " 'چنان خوش بخسبد که سلطان شام',\n",
       " 'غم و شادمانی به سر می\\u200cرود',\n",
       " 'به مرگ این دو از سر به در می\\u200cرود']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boostan = ''\n",
    "\n",
    "for file in boostan_file_name:\n",
    "    fi = open(\"./Boostan/\"+file, \"r\", encoding=\"utf-8\")\n",
    "    text = fi.read()\n",
    "    boostan += text\n",
    "    boostan += \"\\n\"\n",
    "\n",
    "boostan_poems = boostan.split(\"\\n\")\n",
    "boostan_poems[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start preprocessing Boostan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize boostan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مگو جاهی از سلطنت بیش نیست',\n",
       " 'که ایمن\\u200cتر از ملک درویش نیست',\n",
       " 'سبکبار مردم سبک\\u200cتر روند',\n",
       " 'حق این است و صاحبدلان بشنوند',\n",
       " 'تهیدست تشویش نانی خورد',\n",
       " 'جهانبان به قدر جهانی خورد',\n",
       " 'گدا را چو حاصل شود نان شام',\n",
       " 'چنان خوش بخسبد که سلطان شام',\n",
       " 'غم و شادمانی به سر می\\u200cرود',\n",
       " 'به مرگ این دو از سر به در می\\u200cرود']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "normalized_boostan_poems = []\n",
    "for p in boostan_poems:\n",
    "    normalized_boostan_poems.append(normalizer.normalize(p))\n",
    "normalized_boostan_poems[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatize and remove stopwords boostan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مگو جاه سلطنت', 'ایمن ملک درویش', 'سبکبار مردم سبک', 'حق صاحبدلان شنید#شنو', 'تهیدست تشویش نان خورد#خور', 'جهانبان قدر جهانی خورد#خور', 'گدا چو حاصل نان شام', 'چنان خوش بخسبد سلطان شام', 'غم شادمان سر', 'مرگ سر']\n",
      "[['مگو', 'جاه', 'سلطنت'], ['ایمن', 'ملک', 'درویش'], ['سبکبار', 'مردم', 'سبک'], ['حق', 'صاحبدلان', 'شنید#شنو'], ['تهیدست', 'تشویش', 'نان', 'خورد#خور'], ['جهانبان', 'قدر', 'جهانی', 'خورد#خور'], ['گدا', 'چو', 'حاصل', 'نان', 'شام'], ['چنان', 'خوش', 'بخسبد', 'سلطان', 'شام'], ['غم', 'شادمان', 'سر'], ['مرگ', 'سر']]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lemmatize_boostan_poems = []\n",
    "all_cleaned_tokens = []\n",
    "stopwords = [normalizer.normalize(x.strip()) for x in stopwords_list()]+[\"-\", \"/\", \"ز\", '\\ufeff', \"]\", \"[\", \"?\", \"؟\"]\n",
    "\n",
    "mesra_boostan = []\n",
    "mesra_boostan_tokenized = []\n",
    "\n",
    "for p in normalized_boostan_poems:\n",
    "    all_tokens =  word_tokenize(p)\n",
    "    all_tokens_nonstop = [t for t in (all_tokens) if t not in stopwords]\n",
    "    all_tokens_nonstop = [t for t in (all_tokens_nonstop) if len(t)>1]\n",
    "\n",
    "    all_tokens_lemm = []\n",
    "    for i in all_tokens_nonstop:\n",
    "        all_tokens_lemm.append(lemmatizer.lemmatize(i))\n",
    "    \n",
    "    mesra_boostan.append(' '.join(all_tokens_lemm))\n",
    "    # print(all_tokens_nonstop[0])\n",
    "    mesra_boostan_tokenized.append(all_tokens_lemm)\n",
    "\n",
    "print(mesra_boostan[0:10])\n",
    "print(mesra_boostan_tokenized[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start preprocessing Golestan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize golestan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['متکلم را تا کسی عیب نگیرد، سخنش صلاح نپذیرد.',\n",
       " 'مشو غره بر حسن  گفتار  خویش',\n",
       " 'به تحسین نادان و پندار خویش',\n",
       " '',\n",
       " 'بر عجز دشمن رحمت مکن که اگر قادر شود بر تو نبخشاید.',\n",
       " 'دشمن چو بینی ناتوان لاف از بروت خود مزن',\n",
       " 'مغزیست در هر استخوان مردیست در هر پیرهن',\n",
       " '',\n",
       " 'زاهدی مهمان پادشاهی بود. چون به طعام بنشستند کمتر از آن خورد که ارادت او بود و چون به نماز برخاستند بیش از آن کرد که عادت او، تا ظن صلاحیت در حق او زیادت کنند.',\n",
       " 'ترسم نرسی به کعبه، ای اعرابی ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "normalized_golestan_poems = []\n",
    "for p in golestan_poems:\n",
    "    normalized_golestan_poems.append(normalizer.normalize(p))\n",
    "normalized_golestan_poems[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['متکلم عیب گرفت#گیر سخن صلاح پذیرفت#پذیر', 'مشو غره حسن گفتار', 'تحسین نادان پندار', '', 'عجز دشمن رحمت مکن قادر نبخشاید', 'دشمن چو بینی ناتوان لاف برو مزن', 'مغزیست استخوان مردیست پیرهن', '', 'زاهد مهمان پادشاه طعام بنشستند کمتر خورد#خور ارادت نماز برخاستند عادت ظن صلاحیت حق زیاد', 'ترسید#ترس رسید#رس کعبه ای اعرابی']\n",
      "[['متکلم', 'عیب', 'گرفت#گیر', 'سخن', 'صلاح', 'پذیرفت#پذیر'], ['مشو', 'غره', 'حسن', 'گفتار'], ['تحسین', 'نادان', 'پندار'], [], ['عجز', 'دشمن', 'رحمت', 'مکن', 'قادر', 'نبخشاید'], ['دشمن', 'چو', 'بینی', 'ناتوان', 'لاف', 'برو', 'مزن'], ['مغزیست', 'استخوان', 'مردیست', 'پیرهن'], [], ['زاهد', 'مهمان', 'پادشاه', 'طعام', 'بنشستند', 'کمتر', 'خورد#خور', 'ارادت', 'نماز', 'برخاستند', 'عادت', 'ظن', 'صلاحیت', 'حق', 'زیاد'], ['ترسید#ترس', 'رسید#رس', 'کعبه', 'ای', 'اعرابی']]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lemmatize_golestan_poems = []\n",
    "all_cleaned_tokens = []\n",
    "stopwords = [normalizer.normalize(x.strip()) for x in stopwords_list()]+[\"-\", \"/\", \"ز\", '\\ufeff', \"]\", \"[\", \"?\", \"؟\"]\n",
    "\n",
    "mesra_golestan = []\n",
    "mesra_golestan_tokenized = []\n",
    "\n",
    "for p in normalized_golestan_poems:\n",
    "    all_tokens =  word_tokenize(p)\n",
    "    all_tokens_nonstop_g = [t for t in (all_tokens) if t not in stopwords]\n",
    "    all_tokens_nonstop_g = [t for t in (all_tokens_nonstop_g) if len(t)>1]\n",
    "\n",
    "    all_tokens_lemm = []\n",
    "    for i in all_tokens_nonstop_g:\n",
    "        all_tokens_lemm.append(lemmatizer.lemmatize(i))\n",
    "    \n",
    "    mesra_golestan.append(' '.join(all_tokens_lemm))\n",
    "    # print(all_tokens_nonstop[0])\n",
    "    mesra_golestan_tokenized.append(all_tokens_lemm)\n",
    "\n",
    "print(mesra_golestan[0:10])\n",
    "print(mesra_golestan_tokenized[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Boolean Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            کوئری داده شده را از نظر وجود و یا عدم وجود کلمات مشترک در داده ها بررسی میکنیم و در نهایت نتیجه حاصل را ذخیره و چاپ میکنیم\n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    normalizer = Normalizer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    stopwords = [normalizer.normalize(x.strip()) for x in stopwords_list()]+[\"-\", \"/\", \"ز\", '\\ufeff', \"]\", \"[\", \"?\", \"؟\"]\n",
    "    query_normalized = normalizer.normalize(query)\n",
    "    all_tokens = word_tokenize(query_normalized)\n",
    "    all_tokens_nonstop = [t for t in (all_tokens) if t not in stopwords]\n",
    "    all_tokens_nonstop = [t for t in (all_tokens_nonstop) if len(t)>1]\n",
    "\n",
    "    all_tokens_lemm = []\n",
    "    for i in all_tokens_nonstop:\n",
    "        all_tokens_lemm.append(lemmatizer.lemmatize(i))\n",
    "        \n",
    "    return ' '.join(all_tokens_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_similarities_boolean(query_boolean, top_k, original_poems, mesra_tokenized):\n",
    "    results = {}\n",
    "    query_tokenized = word_tokenize(preprocess_query(query_boolean))\n",
    "    query_tokenized_set = set(query_tokenized)\n",
    "    for i, mesra in enumerate(mesra_tokenized):\n",
    "        if not query_tokenized_set.isdisjoint(mesra):\n",
    "            results[original_poems[i]] = len(query_tokenized_set.intersection(set(mesra)))\n",
    "    return nlargest(top_k,results,key=results.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_boostan_boolean(query, k):\n",
    "    boolean_similarities = find_similarities_boolean(query, k, boostan_poems, mesra_boostan_tokenized)\n",
    "    counter = 1\n",
    "    if len(boolean_similarities):\n",
    "        for i in boolean_similarities:\n",
    "            result = \"\".join(i)\n",
    "            print(\"top-{} : {}\".format(counter,result))\n",
    "            counter = counter + 1\n",
    "        print(\"{} results found!\".format(len(boolean_similarities)))\n",
    "    else: print(\"no match found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_golestan_boolean(query, k):\n",
    "    boolean_similarities = find_similarities_boolean(query, k, golestan_poems, mesra_golestan_tokenized)\n",
    "    counter = 1\n",
    "    if len(boolean_similarities):\n",
    "        for i in boolean_similarities:\n",
    "            result = \"\".join(i)\n",
    "            print(\"top-{} : {}\".format(counter,result))\n",
    "            counter = counter + 1\n",
    "        print(\"{} results found!\".format(len(boolean_similarities)))\n",
    "    else: print(\"no match found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'خلوص نیت'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-1 : بر آن باش تا هرچه نیت کنی\n",
      "top-2 : که سلطان به شب نیت روزه کرد\n",
      "top-3 : مصالح بیندیش و نیت بپوش\n",
      "top-4 : دوم نیت آور، سوم کف بشوی\n",
      "top-5 : عبادت به اخلاص نیت نکوست\n",
      "5 results found!\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_boostan_boolean(query, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-1 : کسی از متعلقان منش بر حسب واقعه مطلع گردانید که فلان عزم کرده است و نیت جزم که بقیت عمر معتکف نشیند و خاموشی گزیند، تو نیز اگر توانی سر خویش گیر و راه مجانبت پیش. گفتا: به عزت عظیم و صحبت قدیم که دم بر نیارم و قدم بر ندارم مگر آن گه که سخن گفته شود به عادت مألوف و طریق معروف که آزردن دوستان جهل است و کفّارت یمین سهل و خلاف راه صواب است و نقص رای اولوالالباب ذوالفقار علی در نیام و زبان سعدی در کام.\n",
      "top-2 : بعد از مفارقت او عزم کردم و نیت جزم که بقیت زندگانی فرش هوس درنوردم و گرد مجالست نگردم.\n",
      "2 results found!\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_golestan_boolean(query, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TFIDF Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (1.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.21.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.7.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            ابتدا پکیج های مورد نظر خود را ایمپورت میکنیم\n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در این قسمت داده های خود را به ماتریس مورد نیاز تبدیل میکنیم\n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer_golestan = TfidfVectorizer()\n",
    "doc_term_mat_golestan = vectorizer_golestan.fit_transform(mesra_golestan).toarray()\n",
    "words_golestan = vectorizer_golestan.get_feature_names()\n",
    "df_golestan = pd.DataFrame(doc_term_mat_golestan,columns=words_golestan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_boostan = TfidfVectorizer()\n",
    "doc_term_mat_boostan = vectorizer_boostan.fit_transform(mesra_boostan).toarray()\n",
    "words_boostan = vectorizer_boostan.get_feature_names()\n",
    "df_boostan = pd.DataFrame(doc_term_mat_boostan,columns=words_boostan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در این قسمت تابع محاسبه\n",
    "cosine را پیاده سازی میکنیم و شباهت های بین کوئری داده شده و داده های خود را محاسبه میکنیم\n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(X,vectorizor,query,top_k):\n",
    "    # Vectorize the query to the same length as documents\n",
    "    query_vec = vectorizor.transform(query)\n",
    "    # Compute the cosine similarity between query_vec and all the documents\n",
    "    cosine_similarities_amount = cosine_similarity(X, query_vec).flatten()\n",
    "    # Sort the similar documents from the most similar to less similar and return the indices\n",
    "    most_similar_doc_indices = np.argsort(cosine_similarities_amount, axis=0)[:-top_k - 1:-1]\n",
    "    return most_similar_doc_indices, cosine_similarities_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            نزدیک ترین ابیات به کوئری را به وسیله اندیس های ماتریس به دست میاوریم و چاپ میکنیم\n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_similar_documents(cosine_similarities, similar_doc_indices, original_poems):\n",
    "    \"\"\" Prints the most similar documents using indices in the `similar_doc_indices` vector.\"\"\"\n",
    "    counter = 1\n",
    "    for index in similar_doc_indices:\n",
    "        print('Top-{}, Similarity = {}, text = {}'.format(counter, cosine_similarities[index],original_poems[index]))\n",
    "        # print('body: {}, '.format(df[index]))\n",
    "        print()\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_boostan_tfidf(query, k):\n",
    "    similar_doc_indices , cosine_similarities = calculate_similarity(doc_term_mat,vectorizer,query=[query],top_k=k)\n",
    "    show_similar_documents(cosine_similarities,similar_doc_indices, boostan_poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_golestan_tfidf(query, k):\n",
    "    similar_doc_indices , cosine_similarities = calculate_similarity(doc_term_mat,vectorizer,query=[query],top_k=k)\n",
    "    show_similar_documents(cosine_similarities,similar_doc_indices, golestan_poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'خیال'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-1 : خیال محال اندر او مدغم است\n",
      "top-2 : به خواب اندرش پای بند خیال\n",
      "top-3 : خیالش فرو برده دندان به کام\n",
      "top-4 : دگر هر چه دیدم خیالم نمود\n",
      "top-5 : خیال است پنداشتم یا به خواب\n",
      "top-6 : بِدَرَّد یقین پرده‌های خیال\n",
      "top-7 : خیالت دگر گشت و میلی نماند؟\n",
      "top-8 : خیالش خرف کرد و کالیوه رنگ\n",
      "top-9 : خیالش چنان بر سر آشوب کرد\n",
      "top-10 : خیال و غرورش بر آن داشتی\n",
      "10 results found!\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_boostan_boolean(query, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-1 : ای برتر از خیال و قیاس و گمان و وهم\n",
      "top-2 : خاک، مغز سر خیال اندیش\n",
      "top-3 : دماغ بیهده پخت و خیال باطل بست\n",
      "top-4 : گفتم: تصور مرگ از خیال خود به در کن و وهم را بر طبیعت مستولی مگردان که فیلسوفان یونان گفته‌اند مزاج ار چه مستقیم بود، اعتماد بقا را نشاید و مرض گرچه هایل، دلالت کلی بر هلاک نکند. اگر فرمایی طبیبی را بخوانم تا معالجت کند.\n",
      "top-5 : پدر گفت: ای پسر! به مجرد خیال باطل نشاید روی از تربیت ناصحان بگردانیدن و علما را به ضلالت منسوب کردن و در طلب عالم معصوم از فواید علم محروم ماندن، همچو نابینایی که شبی در وحل افتاده بود و می‌گفت: آخر یکی از مسلمانان چراغی فرا راه من دارید. زنی مازحه بشنید و گفت: تو که چراغ نبینی به چراغ چه بینی؟! همچنین مجلس وعظ چو کلبه بزّاز است آنجا تا نقدی ندهی بضاعتی نستانی و اینجا تا ارادتی نیاری سعادتی نبری.\n",
      "top-6 : خیال بست به پیرانه سر که گیرد جفت\n",
      "top-7 : پدر گفت: ای پسر! خیال محال از سر به در کن و پای قناعت در دامن سلامت کش که بزرگان گفته‌اند دولت نه به کوشیدن است، چاره کم جوشیدن است.\n",
      "top-8 : چنین صفتها که بیان کردم ای فرزند در سفر موجب جمعیت خاطر است و داعیهٔ طیب عیش و آن که از این جمله بی بهره است، به خیال باطل در جهان برود و دیگر کسش نام و نشان نشنود.\n",
      "top-9 : دیگر آسودگی مبند خیال\n",
      "top-10 : شنیدم که به دریای مغرب اندر راه مصر بر گرفته بود و خیال فرعونی در سر، حتی اذا ادرَکَهُ الغَرَقُ، بادی مخالف کشتی برآمد.\n",
      "10 results found!\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_golestan_boolean(query, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfromers-Based Model (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.8;\">مدل های امبدینگ مبتنی بر\n",
    "        transformer</h1>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            یکی از روش‌های مناسب امبدینگ، روش‌های مبتنی بر\n",
    "            transformer\n",
    "            ها هستند و به نوعی مدل‌هایی جدید هستند.\n",
    "            به عنوان نمونه می‌توان از مدل زبانی \n",
    "            BERT\n",
    "            استفاده نمود که مدلی مناسب در ترنسفورمرها می‌باشد.\n",
    "            <br>\n",
    "            حال در این زیربخش سعی داریم با استفاده از \n",
    "            BERT\n",
    "            یک امبدینگ مناسب برای آن انجام دهیم و در نهایت هم موتور جستجوگری برای بوستان و گلستان ایجاد نماییم.\n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentence_transformers in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (4.19.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (0.11.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (1.21.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (1.7.0)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (3.3)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (0.1.96)\n",
      "Requirement already satisfied: huggingface-hub in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sentence_transformers) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (4.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.2)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk->sentence_transformers) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (1.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision->sentence_transformers) (8.4.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgiref (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در این مرحله کافیست مدل \n",
    "            BERT\n",
    "            را از\n",
    "            huggingface\n",
    "            لود کنیم و سپس مصراع‌های بوستان و گلستان که قبل‌تر\n",
    "            روی آنها پیش ‌پردازش کرده بودیم،\n",
    "            با استفاده از \n",
    "            BERT\n",
    "            انکود کنیم.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8359, 768)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesra_boostan_embeddings = model.encode(mesra_boostan) \n",
    "mesra_boostan_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3429, 768)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesra_golestan_embeddings = model.encode(mesra_golestan) \n",
    "mesra_golestan_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در مرحله بعد یک\n",
    "            query\n",
    "            تعریف می‌کنیم و به این ترتیب برای سادگی کار یک تابع برای پیش‌پردازش تعریف می‌نماییم.\n",
    "            که این تابع کارش این است که یک کوئری ورودی را می‌گیرد و ابتدا آن را نرمالایز می‌کند، سپس آن را به توکن‌هایی تبدیل می‌کند و فرآیند\n",
    "            lemmatization \n",
    "            را روی آن اعمال می‌کند.\n",
    "            در نهایت دوباره آن را به یک جمله تبدیل می‌کند و بر می‌گرداند.\n",
    "            <br>\n",
    "            در ادامه از این تابع استفاده نموده‌ایم و کوئری‌ای که داشته‌ایم را پیش‌پردازش کرده‌ایم و سپس شباهت \n",
    "            Cosine Similarity\n",
    "            را برای آن محاسبه کرده‌ایم که بردار نهایی خروجی‌ای است که حاصل این ضرب داخلی بردار هاست.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'همان‌ گونه عذرت بباید خواستن تقصیر را'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    normalizer = Normalizer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    stopwords = [normalizer.normalize(x.strip()) for x in stopwords_list()]+[\"-\", \"/\", \"ز\", '\\ufeff', \"]\", \"[\", \"?\", \"؟\"]\n",
    "    query_normalized = normalizer.normalize(query)\n",
    "    all_tokens = word_tokenize(query_normalized)\n",
    "    all_tokens_nonstop = [t for t in (all_tokens) if t not in stopwords]\n",
    "    all_tokens_nonstop = [t for t in (all_tokens_nonstop) if len(t)>1]\n",
    "\n",
    "    all_tokens_lemm = []\n",
    "    for i in all_tokens_nonstop:\n",
    "        all_tokens_lemm.append(lemmatizer.lemmatize(i))\n",
    "        \n",
    "    return ' '.join(all_tokens_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در آخر کافیست یک تابع تعریف کنیم که در یک آرایه به طول نامشخص\n",
    "            بتواند \n",
    "            k\n",
    "            عنصر با بیشترین مقدار آن را بیابد و به همان ترتیب بتواند اشعار متناظر با آنها را به عنوان پاسخ برگرداند.\n",
    "            از طرفی دو تابع با نام‌های\n",
    "            search_in_boostan\n",
    "            و\n",
    "            search_in_golestan\n",
    "            می‌نویسیم که یک \n",
    "            query\n",
    "            و\n",
    "            یک عدد\n",
    "            k\n",
    "            می‌گیرد و به این ترتیب ابتدا کوئری را پیش‌پردازش می‌کند و سپس\n",
    "            فرآیند\n",
    "            embedding\n",
    "            را روی آن انجام می‌دهد و به کمک مدل\n",
    "            BERT\n",
    "            آن را امبد می‌کند و در نهایت هم با استفاده از\n",
    "            Cosine Similarity\n",
    "            سعی می‌کند بردار شباهت آنها را بیابد و با استفاده از تابع کمکی\n",
    "            find_k_most_relevant\n",
    "            خروجی را تولید می‌کند.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_most_relevant(query_similarity_vector, boostan_poems, k):\n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        max_value = max(query_similarity_vector)\n",
    "        max_index = query_similarity_vector.index(max_value)\n",
    "        results.append(boostan_poems[max_index])\n",
    "        query_similarity_vector[max_index] = -1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_boostan_bert(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_embedding = model.encode(query) \n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_embedding],\n",
    "        mesra_boostan_embeddings\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, boostan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['همین نکته بس عذر تقصیر ما',\n",
       " 'کنون بایدت عذر تقصیر گفت',\n",
       " 'ببایدت عذر خطا خواستن',\n",
       " 'حق از بهر باطل نشاید نهفت',\n",
       " 'خورنده که خیرش برآید ز دست',\n",
       " 'تنی محتشم در لباسی حقیر',\n",
       " 'در اقبال و تأیید بوبکر سعد',\n",
       " 'بقیمت تر از نیشکر صد هزار',\n",
       " 'بگفت این قدر ستر و آسایش است',\n",
       " 'همینم بس از بهر بگذاشتن']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_boostan_bert(query, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_golestan_bert(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_embedding = model.encode(query) \n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_embedding],\n",
    "        mesra_golestan_embeddings\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, golestan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['که در طویله نامردمم بباید ساخت',\n",
       " 'و آدمی\\u200cبچه ندارد خبر و عقل و تمیز',\n",
       " 'آنگه به قوت استیناس محبوب از میان تلاطم امواج محبت سر برآورد و گفت:',\n",
       " 'گر التفات خداوندیش بیاراید',\n",
       " 'لئیم الطبع پندارد که خوانیست',\n",
       " 'شرط عقل است صبر تیرانداز',\n",
       " 'تو بر سر قدر خویشتن باش و وقار ',\n",
       " 'بِئس المطاعِمُ حینَ الذُلِّ تَکسِبُها',\n",
       " 'یاران نهایت عجز او بدانستند و سفره پیش آوردند.',\n",
       " 'این مدعیان در طلبش بی خبرانند']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_golestan_bert(query, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.8;\">مدل های امبدینگ مبتنی بر\n",
    "        میانگین وزن‌دار بردارهای تعبیه</h1>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در این قسمت سعی داریم از مدل \n",
    "            FastText\n",
    "            استفاده نماییم و به این ترتیب برای هر کلمه یک \n",
    "            embedding\n",
    "            داریم. \n",
    "            در واقع می‌توان گفت برخلاف روش استفاده از \n",
    "            BERT\n",
    "            که امبدینگ به صورت\n",
    "            wave to vector\n",
    "            بوده است، این روش یک متد\n",
    "            word to vector\n",
    "            است و به این ترتیب برای هر کلمه یک امبدینگ وجود دارد.\n",
    "            <br>\n",
    "            در ادامه ابتدا کتابخانه‌های مورد نیاز را\n",
    "            import\n",
    "            کرده‌ایم و سپس برای\n",
    "            trainig\n",
    "            تمام فایل‌هایی که به عنوان اشعار بوستان داشتیم را با هم تجمیع می‌کنیم و تمام فایل‌های گلستان را هم یکی می‌کنیم.\n",
    "            در مرحله بعد مدل را روی این فایل‌ها \n",
    "            train\n",
    "            می‌کنیم.\n",
    "            در ادامه مدل را ذخیره می‌کنیم که به صورت یک فایل\n",
    "            bin\n",
    "            ذخیره می‌شود.\n",
    "            \n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('boostan_all.txt', mode='wt', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(mesra_boostan))\n",
    "with open('golestan_all.txt', mode='wt', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(mesra_golestan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  1315\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  127377 lr:  0.000000 avg.loss:  3.610380 ETA:   0h 0m 0s\n",
      "Read 0M words\n",
      "Number of words:  953\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   80034 lr:  0.000000 avg.loss:  3.122186 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model_boostan = fasttext.train_unsupervised('boostan_all.txt')\n",
    "model_golestan = fasttext.train_unsupervised('golestan_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_boostan.save_model(\"boostan_skipgram_model.bin\")\n",
    "model_golestan.save_model(\"golestan_skipgram_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در ادامه دو تابع کاربردی تعریف می‌کنیم. یکی برای محاسبه ی \n",
    "            average vector\n",
    "            برای یک مصراع و به این ترتیب یک مصراع را می‌گیرد و می‌تواند تک تک کلمات آن را\n",
    "            امبد کند و در ادامه میانگین آن را محاسبه می‌نماید.\n",
    "            تابع دوم کارش این است که برای تمام مصراع‌های یک شعر این فرآیند را انجام دهد و یک نام فایل و مدل می‌گیرد\n",
    "            و می‌تواند با استفاده از تابع اول برای تک‌تک مصراع‌ها این فرآیند را انجام دهد.\n",
    "            در ادامه این مورد را برای هر دو سند بوستان و گلستان محاسبه می‌نماییم.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_avg_vector(model, mesra):\n",
    "    all_tokens = mesra.split(' ')\n",
    "    tokens_vectors = []\n",
    "    for word in all_tokens:\n",
    "        tokens_vectors.append(model.get_word_vector(word))\n",
    "\n",
    "    return np.mean(np.array(tokens_vectors), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_text_avg_vectors(model, file_path):\n",
    "    mesra_list = []\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        mesra_list = [line.rstrip() for line in lines]\n",
    "\n",
    "    mesra_avg_vectors = []\n",
    "    \n",
    "    for mesra in mesra_list:\n",
    "        mesra_avg_vect = calculate_text_avg_vector(model, mesra)\n",
    "        mesra_avg_vectors.append(mesra_avg_vect)\n",
    "\n",
    "    return mesra_avg_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8358\n"
     ]
    }
   ],
   "source": [
    "boostan_avg_vector = calculate_all_text_avg_vectors(model_boostan, 'boostan_all.txt')\n",
    "print(len(boostan_avg_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3428\n"
     ]
    }
   ],
   "source": [
    "golestan_avg_vector = calculate_all_text_avg_vectors(model_golestan, 'golestan_all.txt')\n",
    "print(len(golestan_avg_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در مرحله آخر کافیست تابع‌هایی برای این جستجو تعریف نماییم و با استفاده از آنها همانند بخش قبل\n",
    "            می‌توانیم یک کوئری و یک \n",
    "            k\n",
    "            بگیریم و \n",
    "            k\n",
    "            نزدیک ترین سند را در پاسخ برگردانیم.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_boostan_fasttext(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_vector = calculate_text_avg_vector(model_boostan, query)\n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_vector],\n",
    "        boostan_avg_vector\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, boostan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_golestan_fasttext(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_vector = calculate_text_avg_vector(model_golestan, query)\n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_vector],\n",
    "        golestan_avg_vector\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, golestan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'همان‌ گونه عذرت بباید خواستن تقصیر را'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ملامت کنی گفتش ای باد دست',\n",
       " 'که پایابم از دست دشمن نماند',\n",
       " 'که خورد اندر آن روز چندان شراب',\n",
       " 'برو عذر تقصیر طاعت بیار',\n",
       " 'زبان در دهان است عذری بیار',\n",
       " 'به شیرین زبانی توان برد گوی',\n",
       " 'به فریاد خواهان باران شدند',\n",
       " 'همه عمر از اینان چه دیدی خوشی',\n",
       " 'نه هر بار خرما توان خورد و برد',\n",
       " 'که دستش تهی باشد و کار، زار']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_boostan_fasttext(query, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['آورده\\u200cاند که سیه را در آن مدت، نفس طالب بود و شهوت غالب؛ مهرش بجنبید و مهرش برداشت. بامدادان که مَلِک کنیزک را جست و نیافت، حکایت بگفتند. خشم گرفت و فرمود تا سیاه را با کنیزک استوار ببندند و از بام جوسق به قعر خندق در اندازند. یکی از وزرای نیک\\u200cمحضر روی شفاعت بر زمین نهاد و گفت: سیاه بیچاره را در این خطایی نیست که سایر بندگان و خدمتکاران به نوازش خداوندی متعودند. گفت: اگر در مفاوضهٔ او شبی تأخیر کردی، چه شدی؟ که من او را افزون از قیمت کنیزک دلداری کردمی. گفت: ای خداوند روی زمین! نشنیده\\u200cای؟:',\n",
       " 'تا عاقبت الامر دلیلش نماند ذلیلش کردم. دست تعدی دراز کرد و بیهده گفتن آغاز و سنت جاهلان است که چون به دلیل از خصم فرو مانند سلسلهٔ خصومت بجنبانند؛ چون آزر بت تراش که به حجت با پسر برنیامد، به جنگش برخاست که لَئِن لَم تَنتَهِ لَاَرجُمَنِّکَ، دشنامم داد، سقطش گفتم. گریبانم درید زنخدانش گرفتم.',\n",
       " 'عذر تقصیر خدمت آوردم ',\n",
       " 'سیه گوش را گفتند تو را ملازمت صحبت شیر به چه وجه اختیار افتاد گفت تا فضله صیدش می\\u200cخورم و از شر دشمنان در پناه صولت او زندگانی می\\u200cکنم گفتندش اکنون که به ظلّ حمایتش در آمدی و به شکر نعمتش اعتراف کردی چرا نزدیک تر نیایی تا به حلقه خاصانت در آرد و از بندگان مخلصت شمارد گفت همچنان از بطش او ایمن نیستم.',\n",
       " 'ارکان دولت و اعیان حضرت وصیت ملک به جای آوردند و تسلیم مفاتیح قلاع و خزاین بدو کردند و مدّتی ملک راند تا بعضی امرای دولت گردن از طاعت او بپیچانیدند و ملوک از هر طرف به منازعت خاستن گرفتند و به مقاومت لشکر آراستن.',\n",
       " 'گفت: ای یار! دست عتاب از دامن روزگارم بدار. بارها در این مصلحت که تو بینی اندیشه کردم و صبر بر جفای او سهل\\u200cتر آید همی که صبر از دیدن او و حکما گویند: دل بر مجاهده نهادن آسان\\u200cتر است که چشم از مشاهده بر گرفتن.',\n",
       " 'چنین صفتها که بیان کردم ای فرزند در سفر موجب جمعیت خاطر است و داعیهٔ طیب عیش و آن که از این جمله بی بهره است، به خیال باطل در جهان برود و دیگر کسش نام و نشان نشنود.',\n",
       " 'یکی از ملوک خراسان محمود سبکتگین را به خواب چنان دید که جمله وجود او ریخته بود و خاک شده مگر چشمان او که همچنان در چشم خانه همی\\u200cگردید و نظر می\\u200cکرد. سایر حکما از تأویل این فرو ماندند مگر درویشی که به جای آورد و گفت: هنوز نگران است که ملکش با دگران است.',\n",
       " 'یکی از وزرا معزول شد و به حلقه درویشان درآمد اثر برکت صحبت ایشان در او سرایت کرد و جمعیت خاطرش دست داد ملک بار دیگر بر او دل خوش کرد و عمل فرمود قبولش نیامد و گفت معزولی به نزد خردمندان بهتر که مشغولی',\n",
       " 'یکی از ملوک عرب رنجور بود در حالت پیری و امید زندگانی قطع کرده که سواری از در درآمد و بشارت داد که فلان قلعه را به دولت خداوند گشادیم و دشمنان اسیر آمدند و سپاه و رعیت آن طرف بجملگی مطیع فرمان گشتند ملک نفسی سرد بر آورد و گفت این مژده مرا نیست دشمنانم راست یعنی وارثان مملکت.']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_golestan_fasttext(query, k)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
