{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.8;\"> بوستان و گلستان سعدی\n",
    "        </h1>\n",
    "        <p dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        در این پروژه موضوع بوستان و گلستان سعدی انتخاب شده است.\n",
    "          <br>\n",
    "         دیتای مورد نیاز برای این پروژه، کتاب بوستان و گلستان سعدی است که از سایت <a href=\"https://ganjoor.net//saadi/\">گنجور</a>\n",
    "          با استفاده از کرالری که نوشتیم و در فایل تمرین است، جمع‌آوری شده است.\n",
    "        <br>\n",
    "        هر حکایت یا شعر از این دو کتاب، در یک فایل .txt ذخیره شده است و در این قسمت داده ها از فایل‌های ذخیره شده، خوانده میشوند.\n",
    "            \n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <p dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "    ابتدا باید پکیج های مورد نیاز را نصب کنیم.\n",
    "</html>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: six in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from nltk) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'E:\\Education\\SUT\\Spring-00-01\\information retrieval\\HW\\MIR-Saadi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: six in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from nltk==3.3->hazm) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'E:\\Education\\SUT\\Spring-00-01\\information retrieval\\HW\\MIR-Saadi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: future in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (0.18.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'E:\\Education\\SUT\\Spring-00-01\\information retrieval\\HW\\MIR-Saadi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'E:\\Education\\SUT\\Spring-00-01\\information retrieval\\HW\\MIR-Saadi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install hazm\n",
    "!pip install future\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import codecs\n",
    "import tqdm\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "from __future__ import unicode_literals\n",
    "import random\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "230"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boostanPath = \"./Boostan\"\n",
    "golestanPath = \"./Golestan\"\n",
    "NumOfBoostanFiles = len([name for name in os.listdir(boostanPath)])\n",
    "NumOfGolestanFiles = len([name for name in os.listdir(golestanPath)])\n",
    "NumOfBoostanFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <p dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        نام فایل های بوستان و گلستان سعدی را در دو آرایه ذخیره میکنیم تا بعدا از آنها استفاده کنیم.\n",
    "\n",
    "</html> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <p dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        فایل های بوستان\n",
    "</html> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Education\\SUT\\Spring-00-01\\information retrieval\\HW\\MIR-Saadi\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Education\\SUT\\Spring-00-01\\information retrieval\\HW\\MIR-Saadi\n",
      "230\n",
      "E:\\Education\\SUT\\Spring-00-01\\information retrieval\\HW\\MIR-Saadi\n"
     ]
    },
    {
     "data": {
      "text/plain": "['bab10sh1.txt',\n 'bab10sh2.txt',\n 'bab10sh3.txt',\n 'bab10sh4.txt',\n 'bab1sh1.txt',\n 'bab1sh10.txt',\n 'bab1sh11.txt',\n 'bab1sh12.txt',\n 'bab1sh13.txt',\n 'bab1sh14.txt']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd()) \n",
    "boostan_folder_path = \"./Boostan\"\n",
    "\n",
    "boostan_file_name = []\n",
    "golestan_file_name = []\n",
    "\n",
    "os.chdir(boostan_folder_path)\n",
    "print(len(os.listdir()))\n",
    "for filename in os.listdir():\n",
    "    if os.path.isfile(filename):\n",
    "        boostan_file_name.append(filename)\n",
    "\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd()) \n",
    "boostan_file_name[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "     <p dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "        فایل های گلستان\n",
    "</html> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Education\\SUT\\Spring-00-01\\information retrieval\\HW\\MIR-Saadi\n",
      "291\n"
     ]
    },
    {
     "data": {
      "text/plain": "['dibache.txt',\n 'gbab1sh1.txt',\n 'gbab1sh10.txt',\n 'gbab1sh11.txt',\n 'gbab1sh12.txt',\n 'gbab1sh13.txt',\n 'gbab1sh14.txt',\n 'gbab1sh15.txt',\n 'gbab1sh16.txt',\n 'gbab1sh17.txt']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd())  \n",
    "golestan_folder_path = \"./Golestan\"\n",
    "os.chdir(golestan_folder_path)\n",
    "print(len(os.listdir()))\n",
    "for filename in os.listdir():\n",
    "    if os.path.isfile(filename):\n",
    "        golestan_file_name.append(filename)\n",
    "os.chdir(\"..\")\n",
    "golestan_file_name[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make array for golestan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['بِسم اللهِ الرَّحمنِ الرَّحیم',\n 'منّت خدای را عز و جل که طاعتش موجب قربت است و به شکر اندرش مزید نعمت. هر نفسی که فرو می رود ممدّ حیات است و چون بر می آید مفرّح ذات. پس در هر نفسی دو نعمت موجود است و بر هر نعمتی شکری واجب.',\n 'از دست و زبان که بر آید',\n 'کز عهدهٔ شکرش به در آید',\n 'اِعملوا آلَ داودَ شکراً وَ قلیلٌ مِن عبادیَ الشکور',\n 'بنده همان به که ز تقصیر خویش',\n 'عذر به درگاه خدای آورد',\n 'ور نه سزاوار خداوندیش',\n 'کس نتواند که به جای آورد',\n 'باران رحمت بی حسابش همه را رسیده و خوان نعمت بی دریغش همه جا کشیده. پردهٔ ناموس بندگان به گناه فاحش ندرد و وظیفهٔ روزی به خطای منکر نبرد.']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golestan = ''\n",
    "\n",
    "for file in golestan_file_name:\n",
    "    f = open(\"./Golestan/\"+file, \"r\", encoding=\"utf-8\")\n",
    "    text = f.read()\n",
    "    golestan += text\n",
    "    golestan += \"\\n\"\n",
    "\n",
    "golestan_poems = golestan.split(\"\\n\")\n",
    "golestan_poems[0:10]\n",
    "# golestan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make array for boostan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['بیا تا برآریم دستی ز دل',\n 'که نتوان برآورد فردا ز گل',\n 'به فصل خزان در نبینی درخت',\n 'که بی برگ ماند ز سرمای سخت',\n 'برآرد تهی دستهای نیاز',\n 'ز رحمت نگردد تهیدست باز',\n 'مپندار از آن در که هرگز نبست',\n 'که نومید گردد بر آورده دست',\n 'قضا خلعتی نامدارش دهد',\n 'قدر میوه در آستینش نهد']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boostan = ''\n",
    "\n",
    "for file in boostan_file_name:\n",
    "    fi = open(\"./Boostan/\"+file, \"r\", encoding=\"utf-8\")\n",
    "    text = fi.read()\n",
    "    boostan += text\n",
    "    boostan += \"\\n\"\n",
    "\n",
    "boostan_poems = boostan.split(\"\\n\")\n",
    "boostan_poems[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start preprocessing Boostan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize boostan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['بیا تا برآریم دستی ز دل',\n 'که نتوان برآورد فردا ز گل',\n 'به فصل خزان در نبینی درخت',\n 'که بی برگ ماند ز سرمای سخت',\n 'برآرد تهی دستهای نیاز',\n 'ز رحمت نگردد تهیدست باز',\n 'مپندار از آن در که هرگز نبست',\n 'که نومید گردد بر آورده دست',\n 'قضا خلعتی نامدارش دهد',\n 'قدر میوه در آستینش نهد']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "normalized_boostan_poems = []\n",
    "for p in boostan_poems:\n",
    "    normalized_boostan_poems.append(normalizer.normalize(p))\n",
    "normalized_boostan_poems[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatize and remove stopwords boostan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['آمد#آ برآریم دست دل', 'توانست#توان برآورد فردا گل', 'فصل خزان دید#بین درخت', 'برگ ماند#مان سرما سخت', 'برآرد تهی دست', 'رحمت گشت#گرد تهیدست', 'مپندار هرگز بست#بند', 'نومید آورده دست', 'قضا خلعت نامدار', 'قدر میوه آستین نهد']\n",
      "[['آمد#آ', 'برآریم', 'دست', 'دل'], ['توانست#توان', 'برآورد', 'فردا', 'گل'], ['فصل', 'خزان', 'دید#بین', 'درخت'], ['برگ', 'ماند#مان', 'سرما', 'سخت'], ['برآرد', 'تهی', 'دست'], ['رحمت', 'گشت#گرد', 'تهیدست'], ['مپندار', 'هرگز', 'بست#بند'], ['نومید', 'آورده', 'دست'], ['قضا', 'خلعت', 'نامدار'], ['قدر', 'میوه', 'آستین', 'نهد']]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lemmatize_boostan_poems = []\n",
    "all_cleaned_tokens = []\n",
    "stopwords = [normalizer.normalize(x.strip()) for x in stopwords_list()]+[\"-\", \"/\", \"ز\", '\\ufeff', \"]\", \"[\", \"?\", \"؟\"]\n",
    "\n",
    "mesra_boostan = []\n",
    "mesra_boostan_tokenized = []\n",
    "\n",
    "for p in normalized_boostan_poems:\n",
    "    all_tokens =  word_tokenize(p)\n",
    "    all_tokens_nonstop = [t for t in (all_tokens) if t not in stopwords]\n",
    "    all_tokens_nonstop = [t for t in (all_tokens_nonstop) if len(t)>1]\n",
    "\n",
    "    all_tokens_lemm = []\n",
    "    for i in all_tokens_nonstop:\n",
    "        all_tokens_lemm.append(lemmatizer.lemmatize(i))\n",
    "    \n",
    "    mesra_boostan.append(' '.join(all_tokens_lemm))\n",
    "    # print(all_tokens_nonstop[0])\n",
    "    mesra_boostan_tokenized.append(all_tokens_lemm)\n",
    "\n",
    "print(mesra_boostan[0:10])\n",
    "print(mesra_boostan_tokenized[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start preprocessing Golestan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize golestan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['بسم الله الرحمن الرحیم',\n 'منت خدای را عز و جل که طاعتش موجب قربت است و به شکر اندرش مزید نعمت. هر نفسی که فرو می\\u200cرود ممد حیات است و چون بر می\\u200cآید مفرح ذات. پس در هر نفسی دو نعمت موجود است و بر هر نعمتی شکری واجب.',\n 'از دست و زبان که بر آید',\n 'کز عهدهٔ شکرش به در آید',\n 'اعملوا آل داود شکرا و قلیل من عبادی الشکور',\n 'بنده همان به که ز تقصیر خویش',\n 'عذر به درگاه خدای آورد',\n 'ور نه سزاوار خداوندیش',\n 'کس نتواند که به جای آورد',\n 'باران رحمت بی حسابش همه را رسیده و خوان نعمت بی دریغش همه جا کشیده. پردهٔ ناموس بندگان به گناه فاحش ندرد و وظیفهٔ روزی به خطای منکر نبرد.']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "normalized_golestan_poems = []\n",
    "for p in golestan_poems:\n",
    "    normalized_golestan_poems.append(normalizer.normalize(p))\n",
    "normalized_golestan_poems[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['بس الله الرحمن الرحیم', 'منت خدا عز جل طاعت قربت شکر اندر مزید نعمت نفس فرو ممد حیات مفرح ذات نفس نعمت موجود نعمت شکر واجب', 'دست زبان آمد#آ', 'کز عهده شکر آمد#آ', 'اعملوا آل داود شکرا قلیل عبادی الشکور', 'بنده تقصیر', 'عذر درگاه خدا', 'ور سزاوار خداوند', 'کس توانست#توان', 'باران رحمت حساب رسیده خوان نعمت دریغ کشیده پرده ناموس بندگان گناه فاحش ندرد وظیفه روز خطا منکر نبرد']\n",
      "[['بس', 'الله', 'الرحمن', 'الرحیم'], ['منت', 'خدا', 'عز', 'جل', 'طاعت', 'قربت', 'شکر', 'اندر', 'مزید', 'نعمت', 'نفس', 'فرو', 'ممد', 'حیات', 'مفرح', 'ذات', 'نفس', 'نعمت', 'موجود', 'نعمت', 'شکر', 'واجب'], ['دست', 'زبان', 'آمد#آ'], ['کز', 'عهده', 'شکر', 'آمد#آ'], ['اعملوا', 'آل', 'داود', 'شکرا', 'قلیل', 'عبادی', 'الشکور'], ['بنده', 'تقصیر'], ['عذر', 'درگاه', 'خدا'], ['ور', 'سزاوار', 'خداوند'], ['کس', 'توانست#توان'], ['باران', 'رحمت', 'حساب', 'رسیده', 'خوان', 'نعمت', 'دریغ', 'کشیده', 'پرده', 'ناموس', 'بندگان', 'گناه', 'فاحش', 'ندرد', 'وظیفه', 'روز', 'خطا', 'منکر', 'نبرد']]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lemmatize_golestan_poems = []\n",
    "all_cleaned_tokens = []\n",
    "stopwords = [normalizer.normalize(x.strip()) for x in stopwords_list()]+[\"-\", \"/\", \"ز\", '\\ufeff', \"]\", \"[\", \"?\", \"؟\"]\n",
    "\n",
    "mesra_golestan = []\n",
    "mesra_golestan_tokenized = []\n",
    "\n",
    "for p in normalized_golestan_poems:\n",
    "    all_tokens =  word_tokenize(p)\n",
    "    all_tokens_nonstop_g = [t for t in (all_tokens) if t not in stopwords]\n",
    "    all_tokens_nonstop_g = [t for t in (all_tokens_nonstop_g) if len(t)>1]\n",
    "\n",
    "    all_tokens_lemm = []\n",
    "    for i in all_tokens_nonstop_g:\n",
    "        all_tokens_lemm.append(lemmatizer.lemmatize(i))\n",
    "    \n",
    "    mesra_golestan.append(' '.join(all_tokens_lemm))\n",
    "    # print(all_tokens_nonstop[0])\n",
    "    mesra_golestan_tokenized.append(all_tokens_lemm)\n",
    "\n",
    "print(mesra_golestan[0:10])\n",
    "print(mesra_golestan_tokenized[0:10])\n"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "TFIDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'E:\\Education\\SUT\\Spring-00-01\\information retrieval\\HW\\MIR-Saadi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: joblib>=1.0.0 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from scikit-learn) (1.8.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'E:\\Education\\SUT\\Spring-00-01\\information retrieval\\HW\\MIR-Saadi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "doc_term_mat = vectorizer.fit_transform(mesra_golestan).toarray()\n",
    "words = vectorizer.get_feature_names()\n",
    "df = pd.DataFrame(doc_term_mat,columns=words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "def calculate_similarity(X,vectorizor,query,top_k):\n",
    "    # Vectorize the query to the same length as documents\n",
    "    query_vec = vectorizor.transform(query)\n",
    "    # Compute the cosine similarity between query_vec and all the documents\n",
    "    cosine_similarities = cosine_similarity(X,query_vec).flatten()\n",
    "    # Sort the similar documents from the most similar to less similar and return the indices\n",
    "    most_similar_doc_indices = np.argsort(cosine_similarities, axis=0)[:-top_k-1:-1]\n",
    "    return most_similar_doc_indices, cosine_similarities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "def show_similar_documents( cosine_similarities, similar_doc_indices):\n",
    "    \"\"\" Prints the most similar documents using indices in the `similar_doc_indices` vector.\"\"\"\n",
    "    counter = 1\n",
    "    for index in similar_doc_indices:\n",
    "        print('Top-{}, Similarity = {}, text = {}'.format(counter, cosine_similarities[index],mesra_golestan[index]))\n",
    "        # print('body: {}, '.format(df[index]))\n",
    "        print()\n",
    "        counter += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1, Similarity = 0.4911106545635505, text = آسودگی مبند خیال\n",
      "\n",
      "Top-2, Similarity = 0.48519477914728837, text = خاک مغز سر خیال اندیش\n",
      "\n",
      "Top-3, Similarity = 0.41085306751277245, text = خیال بست#بند پیرانه سر جفت\n",
      "\n",
      "Top-4, Similarity = 0.40081229807614704, text = ای برتر خیال قیاس گمان وهم\n",
      "\n",
      "Top-5, Similarity = 0.3324660360912388, text = دماغ بیهده پخت#پز خیال باطل بست#بند\n",
      "\n",
      "Top-6, Similarity = 0.2859122721226928, text = دوست پادشاه اعتماد توانست#توان آواز خوش کودک خیال مبدل خوابید#خواب متغیر\n",
      "\n",
      "Top-7, Similarity = 0.2721994405074933, text = باری نصیحت گفت#گو خیال محال تجنب کن خلق بدین هوس داشت#دار اسیرند پای زنجیر\n",
      "\n",
      "Top-8, Similarity = 0.2604998267087051, text = پدر ای پسر خیال محال سر کن پای قناعت دامن سلامت کش بزرگ گفت#گو دولت کوشیدن چاره جوشیدن\n",
      "\n",
      "Top-9, Similarity = 0.24003471169184967, text = شنید#شنو دریا مغرب اندر مصر گرفت#گیر خیال فرعون سر اذا ادرکه الغرق باد مخالف کشتی برآمد\n",
      "\n",
      "Top-10, Similarity = 0.22703495579655641, text = صفت ای فرزند سفر جمعیت خاطر داعیه طیب عیش جمله بهره خیال باطل جهان رفت#رو کس نام نشان شنید#شنو\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_doc_indices , cosine_similarities = calculate_similarity(doc_term_mat,vectorizer,query=[\"خیال\"],top_k=10)\n",
    "show_similar_documents(cosine_similarities,similar_doc_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfromers-Based Model (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.8;\">مدل های امبدینگ مبتنی بر\n",
    "        transformer</h1>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            یکی از روش‌های مناسب امبدینگ، روش‌های مبتنی بر\n",
    "            transformer\n",
    "            ها هستند و به نوعی مدل‌هایی جدید هستند.\n",
    "            به عنوان نمونه می‌توان از مدل زبانی \n",
    "            BERT\n",
    "            استفاده نمود که مدلی مناسب در ترنسفورمرها می‌باشد.\n",
    "            <br>\n",
    "            حال در این زیربخش سعی داریم با استفاده از \n",
    "            BERT\n",
    "            یک امبدینگ مناسب برای آن انجام دهیم و در نهایت هم موتور جستجوگری برای بوستان و گلستان ایجاد نماییم.\n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "Requirement already satisfied: tqdm in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from sentence_transformers) (4.64.0)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.11.0-cp38-cp38-win_amd64.whl (158.0 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: numpy in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from sentence_transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from sentence_transformers) (1.1.1)\n",
      "Requirement already satisfied: scipy in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from sentence_transformers) (1.8.1)\n",
      "Requirement already satisfied: nltk in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from sentence_transformers) (3.3)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: six in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from nltk->sentence_transformers) (1.16.0)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.5.18.1-py3-none-any.whl (155 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in e:\\education\\sut\\spring-00-01\\information retrieval\\hw\\mir-saadi\\venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.1.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Using cached Pillow-9.1.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=3a7ca31a334088994d6fd3607fd5c01600d963de65ebb4905d3ca91b0bb34d17\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\0c\\b6\\fb\\2289a932c365293ad865fc1fe9d2db694d5584241c6d670874\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, typing-extensions, requests, pyyaml, filelock, torch, tokenizers, pillow, huggingface-hub, transformers, torchvision, sentencepiece, sentence-transformers\n",
      "Successfully installed certifi-2022.5.18.1 charset-normalizer-2.0.12 filelock-3.7.1 huggingface-hub-0.7.0 idna-3.3 pillow-9.1.1 pyyaml-6.0 requests-2.27.1 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 torch-1.11.0 torchvision-0.12.0 transformers-4.19.2 typing-extensions-4.2.0 urllib3-1.26.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'E:\\Education\\SUT\\Spring-00-01\\information retrieval\\HW\\MIR-Saadi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در این مرحله کافیست مدل \n",
    "            BERT\n",
    "            را از\n",
    "            huggingface\n",
    "            لود کنیم و سپس مصراع‌های بوستان و گلستان که قبل‌تر\n",
    "            روی آنها پیش ‌پردازش کرده بودیم،\n",
    "            با استفاده از \n",
    "            BERT\n",
    "            انکود کنیم.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/391 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1f0d2d920a0416fa55f2ea6a25a674e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f31cb3680df42a98a5458c873d6ed35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/3.95k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71952cc9426e4773a2562c026acf5971"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4af47da3fadc4d0795b0e73ab2719314"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16d7a7b25185460d9f91612855ddf71d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83daf439524b46a2801fea8fc4cd539e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2a9980bfe5c4e668b1a006d99c811e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b226ac2ec6a1440187761a4e5753b744"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6388c6e38334115b39f3d106940211c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94070f1f10ba4849b7c01d77ea715715"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a294476b0aa149d69f6bc55ddabbff83"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/399 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf1543c000a6440ba8b300e3e9c93892"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20010dd4893c43acb33d3dc73b941b2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(8359, 768)"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesra_boostan_embeddings = model.encode(mesra_boostan) \n",
    "mesra_boostan_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3429, 768)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesra_golestan_embeddings = model.encode(mesra_golestan) \n",
    "mesra_golestan_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در مرحله بعد یک\n",
    "            query\n",
    "            تعریف می‌کنیم و به این ترتیب برای سادگی کار یک تابع برای پیش‌پردازش تعریف می‌نماییم.\n",
    "            که این تابع کارش این است که یک کوئری ورودی را می‌گیرد و ابتدا آن را نرمالایز می‌کند، سپس آن را به توکن‌هایی تبدیل می‌کند و فرآیند\n",
    "            lemmatization \n",
    "            را روی آن اعمال می‌کند.\n",
    "            در نهایت دوباره آن را به یک جمله تبدیل می‌کند و بر می‌گرداند.\n",
    "            <br>\n",
    "            در ادامه از این تابع استفاده نموده‌ایم و کوئری‌ای که داشته‌ایم را پیش‌پردازش کرده‌ایم و سپس شباهت \n",
    "            Cosine Similarity\n",
    "            را برای آن محاسبه کرده‌ایم که بردار نهایی خروجی‌ای است که حاصل این ضرب داخلی بردار هاست.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'همان‌ گونه عذرت بباید خواستن تقصیر را'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    normalizer = Normalizer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    stopwords = [normalizer.normalize(x.strip()) for x in stopwords_list()]+[\"-\", \"/\", \"ز\", '\\ufeff', \"]\", \"[\", \"?\", \"؟\"]\n",
    "    query_normalized = normalizer.normalize(query)\n",
    "    all_tokens = word_tokenize(query_normalized)\n",
    "    all_tokens_nonstop = [t for t in (all_tokens) if t not in stopwords]\n",
    "    all_tokens_nonstop = [t for t in (all_tokens_nonstop) if len(t)>1]\n",
    "\n",
    "    all_tokens_lemm = []\n",
    "    for i in all_tokens_nonstop:\n",
    "        all_tokens_lemm.append(lemmatizer.lemmatize(i))\n",
    "        \n",
    "    return ' '.join(all_tokens_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در آخر کافیست یک تابع تعریف کنیم که در یک آرایه به طول نامشخص\n",
    "            بتواند \n",
    "            k\n",
    "            عنصر با بیشترین مقدار آن را بیابد و به همان ترتیب بتواند اشعار متناظر با آنها را به عنوان پاسخ برگرداند.\n",
    "            از طرفی دو تابع با نام‌های\n",
    "            search_in_boostan\n",
    "            و\n",
    "            search_in_golestan\n",
    "            می‌نویسیم که یک \n",
    "            query\n",
    "            و\n",
    "            یک عدد\n",
    "            k\n",
    "            می‌گیرد و به این ترتیب ابتدا کوئری را پیش‌پردازش می‌کند و سپس\n",
    "            فرآیند\n",
    "            embedding\n",
    "            را روی آن انجام می‌دهد و به کمک مدل\n",
    "            BERT\n",
    "            آن را امبد می‌کند و در نهایت هم با استفاده از\n",
    "            Cosine Similarity\n",
    "            سعی می‌کند بردار شباهت آنها را بیابد و با استفاده از تابع کمکی\n",
    "            find_k_most_relevant\n",
    "            خروجی را تولید می‌کند.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_most_relevant(query_similarity_vector, boostan_poems, k):\n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        max_value = max(query_similarity_vector)\n",
    "        max_index = query_similarity_vector.index(max_value)\n",
    "        results.append(boostan_poems[max_index])\n",
    "        query_similarity_vector[max_index] = -1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_boostan(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_embedding = model.encode(query) \n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_embedding],\n",
    "        mesra_boostan_embeddings\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, boostan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['همین نکته بس عذر تقصیر ما',\n",
       " 'ببایدت عذر خطا خواستن',\n",
       " 'کنون بایدت عذر تقصیر گفت',\n",
       " 'بگفت این قدر ستر و آسایش است',\n",
       " 'برو عذر تقصیر طاعت بیار',\n",
       " 'در اقبال و تأیید بوبکر سعد',\n",
       " 'خورنده که خیرش برآید ز دست',\n",
       " 'براندیش از افتان و خیزان تب',\n",
       " 'که فرزند خویشت برآید تباه',\n",
       " 'حق از بهر باطل نشاید نهفت']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_boostan(query, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_golestan(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_embedding = model.encode(query) \n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_embedding],\n",
    "        mesra_golestan_embeddings\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, golestan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['بنده همان به که ز تقصیر خویش',\n",
       " 'تو بر سر قدر خویشتن باش و وقار ',\n",
       " 'گر التفات خداوندیش بیاراید',\n",
       " 'و آدمی\\u200cبچه ندارد خبر و عقل و تمیز',\n",
       " 'که در طویله نامردمم بباید ساخت',\n",
       " 'افلاس عنان از کف تقوی بستاند',\n",
       " 'نترسد آن که بر افتادگان نبخشاید',\n",
       " 'که هر خاری به تسبیحش زبانیست',\n",
       " 'گفت: ترسم که بینا شود و دخترم را طلاق دهد.',\n",
       " 'ور وزیر از خدا بترسیدی ']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_golestan(query, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.8;\">مدل های امبدینگ مبتنی بر\n",
    "        میانگین وزن‌دار بردارهای تعبیه</h1>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در این قسمت سعی داریم از مدل \n",
    "            FastText\n",
    "            استفاده نماییم و به این ترتیب برای هر کلمه یک \n",
    "            embedding\n",
    "            داریم. \n",
    "            در واقع می‌توان گفت برخلاف روش استفاده از \n",
    "            BERT\n",
    "            که امبدینگ به صورت\n",
    "            wave to vector\n",
    "            بوده است، این روش یک متد\n",
    "            word to vector\n",
    "            است و به این ترتیب برای هر کلمه یک امبدینگ وجود دارد.\n",
    "            <br>\n",
    "            در ادامه ابتدا کتابخانه‌های مورد نیاز را\n",
    "            import\n",
    "            کرده‌ایم و سپس برای\n",
    "            trainig\n",
    "            تمام فایل‌هایی که به عنوان اشعار بوستان داشتیم را با هم تجمیع می‌کنیم و تمام فایل‌های گلستان را هم یکی می‌کنیم.\n",
    "            در مرحله بعد مدل را روی این فایل‌ها \n",
    "            train\n",
    "            می‌کنیم.\n",
    "            در ادامه مدل را ذخیره می‌کنیم که به صورت یک فایل\n",
    "            bin\n",
    "            ذخیره می‌شود.\n",
    "            \n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('boostan_all.txt', mode='wt', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(mesra_boostan))\n",
    "with open('golestan_all.txt', mode='wt', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(mesra_golestan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  1315\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  127933 lr:  0.000000 avg.loss:  3.607142 ETA:   0h 0m 0s\n",
      "Read 0M words\n",
      "Number of words:  953\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   83931 lr:  0.000000 avg.loss:  3.138286 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model_boostan = fasttext.train_unsupervised('boostan_all.txt')\n",
    "model_golestan = fasttext.train_unsupervised('golestan_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_boostan.save_model(\"boostan_skipgram_model.bin\")\n",
    "model_golestan.save_model(\"golestan_skipgram_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در ادامه دو تابع کاربردی تعریف می‌کنیم. یکی برای محاسبه ی \n",
    "            average vector\n",
    "            برای یک مصراع و به این ترتیب یک مصراع را می‌گیرد و می‌تواند تک تک کلمات آن را\n",
    "            امبد کند و در ادامه میانگین آن را محاسبه می‌نماید.\n",
    "            تابع دوم کارش این است که برای تمام مصراع‌های یک شعر این فرآیند را انجام دهد و یک نام فایل و مدل می‌گیرد\n",
    "            و می‌تواند با استفاده از تابع اول برای تک‌تک مصراع‌ها این فرآیند را انجام دهد.\n",
    "            در ادامه این مورد را برای هر دو سند بوستان و گلستان محاسبه می‌نماییم.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_avg_vector(model, mesra):\n",
    "    all_tokens = mesra.split(' ')\n",
    "    tokens_vectors = []\n",
    "    for word in all_tokens:\n",
    "        tokens_vectors.append(model.get_word_vector(word))\n",
    "\n",
    "    return np.mean(np.array(tokens_vectors), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_text_avg_vectors(model, file_path):\n",
    "    mesra_list = []\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        mesra_list = [line.rstrip() for line in lines]\n",
    "\n",
    "    mesra_avg_vectors = []\n",
    "    \n",
    "    for mesra in mesra_list:\n",
    "        mesra_avg_vect = calculate_text_avg_vector(model, mesra)\n",
    "        mesra_avg_vectors.append(mesra_avg_vect)\n",
    "\n",
    "    return mesra_avg_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8358\n"
     ]
    }
   ],
   "source": [
    "boostan_avg_vector = calculate_all_text_avg_vectors(model_boostan, 'boostan_all.txt')\n",
    "print(len(boostan_avg_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3428\n"
     ]
    }
   ],
   "source": [
    "golestan_avg_vector = calculate_all_text_avg_vectors(model_golestan, 'golestan_all.txt')\n",
    "print(len(golestan_avg_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> \n",
    "    <head>\n",
    "        <link rel=\"preconnect\" href=\"//fdn.fontcdn.ir\">\n",
    "        <link rel=\"preconnect\" href=\"//v1.fontapi.ir\">\n",
    "        <link href=\"https://v1.fontapi.ir/css/Vazir\" rel=\"stylesheet\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h3 dir=\"rtl\" style=\"font-family: 'Vazir', sans-serif; line-height: 1.6;\">\n",
    "            در مرحله آخر کافیست تابع‌هایی برای این جستجو تعریف نماییم و با استفاده از آنها همانند بخش قبل\n",
    "            می‌توانیم یک کوئری و یک \n",
    "            k\n",
    "            بگیریم و \n",
    "            k\n",
    "            نزدیک ترین سند را در پاسخ برگردانیم.\n",
    "            \n",
    "<!--        </h3>\n",
    "    </body> -->\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_boostan_fasttext(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_vector = calculate_text_avg_vector(model_boostan, query)\n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_vector],\n",
    "        boostan_avg_vector\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, boostan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_golestan_fasttext(query, k):\n",
    "    query = preprocess_query(query)\n",
    "    query_vector = calculate_text_avg_vector(model_golestan, query)\n",
    "    query_similarity_vector = cosine_similarity(\n",
    "        [query_vector],\n",
    "        golestan_avg_vector\n",
    "    )\n",
    "    query_similarity_vector = list(query_similarity_vector[0])\n",
    "    return find_k_most_relevant(query_similarity_vector, golestan_poems, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'همان‌ گونه عذرت بباید خواستن تقصیر را'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ملامت کنی گفتش ای باد دست',\n",
       " 'برو عذر تقصیر طاعت بیار',\n",
       " 'زبان در دهان است عذری بیار',\n",
       " 'به فریاد خواهان باران شدند',\n",
       " 'که خورد اندر آن روز چندان شراب',\n",
       " 'به شیرین زبانی توان برد گوی',\n",
       " 'نه هر بار خرما توان خورد و برد',\n",
       " 'که پایابم از دست دشمن نماند',\n",
       " 'همه عمر از اینان چه دیدی خوشی',\n",
       " 'به چندان که در دستت افتد بساز']"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "search_in_boostan_fasttext(query, k)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}